ARG PY_SPARK_VERSION

FROM py-spark-spark:${PY_SPARK_VERSION} AS build

FROM python:3.11-slim-bullseye AS runtime

# need to create the same user (spark) with the same UID (185) like any apache/spark:4.0.0-python3 image
# Create a group named 'spark' with GID 185
RUN groupadd --gid 185 spark
RUN useradd --uid 185 --gid 185 --create-home --shell /bin/sh spark
RUN usermod -aG root spark

COPY --chown=spark:spark --from=build /opt/spark/ /opt/spark/
COPY --chown=spark:spark --from=build /home/spark/ /home/spark/
COPY --chown=spark:spark --from=build /spark/ /spark/

ENV PIP_DEFAULT_TIMEOUT=180
ENV SPARK_HOME=/opt/spark
ENV PATH=$PATH:$SPARK_HOME/bin

RUN apt-get update \
    && apt-get dist-upgrade -y \
    && apt-get install -y --no-install-recommends openjdk-17-jdk wget curl procps openssh-server net-tools iputils-ping vim \
    && apt-get clean \
    && rm -rf \
      /var/lib/apt/lists/* \
      /tmp/* \
      /var/tmp/*

RUN python -m pip install --no-cache-dir dbt-core dbt-spark[PyHive] dbt-spark[session]

RUN python -m pip install --no-cache-dir pyspark==4.0.0

# ---- the following part don't use in production, since it's only for dev environment. It compromises security in many way
# using generating public/private key connection for dev environment
# Create .ssh directory and set permissions
RUN mkdir -p /root/.ssh && chmod 700 /root/.ssh && mkdir -p /run/sshd
COPY ./dockerkey.pub /root/.ssh/authorized_keys
# Set correct permissions for SSH key
RUN chmod 600 /root/.ssh/authorized_keys
RUN sed -i 's/#PubkeyAuthentication yes/PubkeyAuthentication yes/' /etc/ssh/sshd_config && \
    sed -i 's/#PasswordAuthentication yes/PasswordAuthentication no/' /etc/ssh/sshd_config && \
    sed -i 's/UsePAM yes/UsePAM no/' /etc/ssh/sshd_config && \
    sed -i 's/#PermitRootLogin prohibit-password/PermitRootLogin yes/' /etc/ssh/sshd_config && \
    sed -i 's/#PermitUserEnvironment no/PermitUserEnvironment yes/' /etc/ssh/sshd_config
# ---- end of permissions part


USER spark

COPY --chown=spark:spark /dbt /dbt

COPY --chown=spark:spark scripts/start_dbt.sh /start_dbt.sh

WORKDIR /opt/spark

USER root

ENTRYPOINT ["sh", "-c", "/start_dbt.sh"]

EXPOSE 22
