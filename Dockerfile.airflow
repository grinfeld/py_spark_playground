ARG PY_SPARK_VERSION

FROM py-spark-spark:${PY_SPARK_VERSION} AS build

# Use Apache Airflow 3.1.0 as base
FROM apache/airflow:3.1.0-python3.11 AS runtime

COPY --chown=airflow:root --from=build /opt/spark/ /opt/spark/
COPY --chown=airflow:root --from=build /home/spark/ /home/spark/
COPY --chown=airflow:root --from=build /spark/ /spark/

# Set environment variables
ENV PYTHONDONTWRITEBYTECODE=1
ENV SPARK_HOME=/opt/spark
ENV PATH=$PATH:$SPARK_HOME/bin

# Install system dependencies
USER root
RUN apt-get update && apt-get install -y \
    openjdk-17-jdk \
    wget \
    curl \
    net-tools iputils-ping \
    openssh-client \
    && rm -rf /var/lib/apt/lists/*

# Create .ssh directory for airflow user, set ownership/permissions, copy key, and set key permissions
RUN mkdir -p /home/airflow/.ssh && \
    chown airflow:root /home/airflow/.ssh && \
    chmod 700 /home/airflow/.ssh
COPY --chown=airflow:root dockerkey /home/airflow/.ssh/id_rsa
RUN chmod 600 /home/airflow/.ssh/id_rsa

USER airflow

COPY dags/ "/opt/airflow/dags"

# Install Python dependencies
COPY requirements.txt /tmp/requirements.txt
RUN pip install --no-cache-dir -r /tmp/requirements.txt

# Create directories for logs
RUN mkdir -p /opt/airflow/logs

COPY helm/templates/dbt_template.yaml "/dbt/templates/dbt_template.yaml"

# Set working directory
WORKDIR /opt/airflow
