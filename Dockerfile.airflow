# Use Apache Airflow 3.0.6 as base
FROM apache/airflow:3.0.6-python3.11

# Set environment variables
ENV PYTHONDONTWRITEBYTECODE=1
ENV SPARK_HOME=/opt/spark
ENV PATH=$PATH:$SPARK_HOME/bin

# Install system dependencies
USER root
RUN apt-get update && apt-get install -y \
    openjdk-17-jdk \
    wget \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Download and install Apache Spark 3.5.6
RUN wget -q https://archive.apache.org/dist/spark/spark-3.5.6/spark-3.5.6-bin-hadoop3.tgz \
    && tar -xzf spark-3.5.6-bin-hadoop3.tgz \
    && mv spark-3.5.6-bin-hadoop3 /opt/spark/ \
    && rm spark-3.5.6-bin-hadoop3.tgz

USER airflow

# Install Python dependencies
COPY requirements.txt /tmp/requirements.txt
RUN pip install --no-cache-dir -r /tmp/requirements.txt

# Create directories for logs
RUN mkdir -p /opt/airflow/logs

# RUN mkdir -p /tmp/sparks/
# RUN mkdir -p /opt/airflow/packages

# COPY sparks/ /tmp/sparks/

COPY spark-defaults.conf "/opt/spark/conf/spark-defaults.conf"

# RUN cd /tmp/sparks/ && python -m build
# RUN cp /tmp/sparks/dist/sparks*.whl /opt/airflow/packages/sparks.whl

# Set working directory
WORKDIR /opt/airflow
