version: '3.8'

services:
  # MinIO Object Storage
  minio:
    image: minio/minio:latest
    container_name: minio-distributed
    ports:
      - "9000:9000"      # API port
      - "9001:9001"      # Console port
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    command: server /data --console-address ":9001"
    volumes:
      - minio_data:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3
    networks:
      - spark-network
    restart: unless-stopped

  # MinIO Client for setup
  minio-client:
    image: minio/mc:latest
    container_name: minio-client-distributed
    depends_on:
      - minio
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    command: >
      sh -c "
        sleep 10 &&
        mc alias set myminio http://minio:9000 minioadmin minioadmin &&
        mc mb myminio/spark-data &&
        mc mb myminio/spark-output &&
        mc mb myminio/spark-checkpoints &&
        mc policy set public myminio/spark-data &&
        mc policy set public myminio/spark-output &&
        mc policy set public myminio/spark-checkpoints &&
        echo 'MinIO setup completed'
      "
    networks:
      - spark-network

  # Spark Master
  spark-master:
    build:
      context: .
      dockerfile: Dockerfile.spark
    container_name: spark-master
    depends_on:
      - minio
      - minio-client
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - STORAGE_TYPE=minio
      - STORAGE_ENDPOINT=http://minio:9000
      - STORAGE_ACCESS_KEY_ID=minioadmin
      - STORAGE_SECRET_KEY=minioadmin
      - STORAGE_BUCKET=spark-data
    ports:
      - "8080:8080"  # Spark Master UI
      - "7077:7077"  # Spark Master RPC
    volumes:
      - ./data:/app/data:ro
      - ./output:/app/output
      - spark_logs:/opt/spark/logs
      - spark_work:/opt/spark/work
    networks:
      - spark-network
    restart: unless-stopped
    command: >
      sh -c "
        sleep 15 &&
        /opt/spark/sbin/start-master.sh &&
        tail -f /opt/spark/logs/spark-*-org.apache.spark.deploy.master.Master-*.out
      "

  # Spark Worker 1
  spark-worker-1:
    build:
      context: .
      dockerfile: Dockerfile.spark
    container_name: spark-worker-1
    depends_on:
      - spark-master
      - minio
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - STORAGE_TYPE=minio
      - STORAGE_ENDPOINT=http://minio:9000
      - STORAGE_ACCESS_KEY_ID=minioadmin
      - STORAGE_SECRET_KEY=minioadmin
    volumes:
      - ./data:/app/data:ro
      - ./output:/app/output
      - spark_logs:/opt/spark/logs
      - spark_work:/opt/spark/work
    networks:
      - spark-network
    restart: unless-stopped
    command: >
      sh -c "
        sleep 20 &&
        /opt/spark/sbin/start-worker.sh spark://spark-master:7077 &&
        tail -f /opt/spark/logs/spark-*-org.apache.spark.deploy.worker.Worker-*.out
      "

  # Spark Worker 2
  spark-worker-2:
    build:
      context: .
      dockerfile: Dockerfile.spark
    container_name: spark-worker-2
    depends_on:
      - spark-master
      - minio
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - STORAGE_TYPE=minio
      - STORAGE_ENDPOINT=http://minio:9000
      - STORAGE_ACCESS_KEY_ID=minioadmin
      - STORAGE_SECRET_KEY=minioadmin
    volumes:
      - ./data:/app/data:ro
      - ./output:/app/output
      - spark_logs:/opt/spark/logs
      - spark_work:/opt/spark/work
    networks:
      - spark-network
    restart: unless-stopped
    command: >
      sh -c "
        sleep 20 &&
        /opt/spark/sbin/start-worker.sh spark://spark-master:7077 &&
        tail -f /opt/spark/logs/spark-*-org.apache.spark.deploy.worker.Worker-*.out
      "

  # Spark Worker 3
  spark-worker-3:
    build:
      context: .
      dockerfile: Dockerfile.spark
    container_name: spark-worker-3
    depends_on:
      - spark-master
      - minio
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - STORAGE_TYPE=minio
      - STORAGE_ENDPOINT=http://minio:9000
      - STORAGE_ACCESS_KEY_ID=minioadmin
      - STORAGE_SECRET_KEY=minioadmin
    volumes:
      - ./data:/app/data:ro
      - ./output:/app/output
      - spark_logs:/opt/spark/logs
      - spark_work:/opt/spark/work
    networks:
      - spark-network
    restart: unless-stopped
    command: >
      sh -c "
        sleep 20 &&
        /opt/spark/sbin/start-worker.sh spark://spark-master:7077 &&
        tail -f /opt/spark/logs/spark-*-org.apache.spark.deploy.worker.Worker-*.out
      "

  # Hive Metastore for Iceberg
  hive-metastore:
    image: apache/hive:3.1.3
    container_name: hive-metastore-distributed
    depends_on:
      - minio
    environment:
      - SERVICE_NAME=metastore
      - DB_DRIVER=derby
      - DB_DRIVER_CLASS=org.apache.derby.jdbc.EmbeddedDriver
      - DB_DRIVER_URL=jdbc:derby:memory:metastore_db;create=true
      - DB_USERNAME=APP
      - DB_PASSWORD=mine
    ports:
      - "9083:9083"
    volumes:
      - hive_metastore_db:/opt/hive/data
    networks:
      - spark-network
    command: ["/opt/hive/bin/hive", "--service", "metastore"]

  # Spark Application Driver
  spark-app:
    build:
      context: .
      dockerfile: Dockerfile.spark
    container_name: spark-app-distributed
    depends_on:
      - spark-master
      - spark-worker-1
      - spark-worker-2
      - spark-worker-3
      - minio
      - minio-client
      - hive-metastore
    environment:
      # Storage Configuration
      - STORAGE_ENDPOINT=${STORAGE_ENDPOINT}
      - STORAGE_ACCESS_KEY_ID=${STORAGE_ACCESS_KEY_ID}
      - STORAGE_SECRET_KEY=${STORAGE_SECRET_KEY}
      - STORAGE_CREDENTIALS_PROVIDER=${STORAGE_CREDENTIALS_PROVIDER}
      - STORAGE_PATH_STYLE_ACCESS=${STORAGE_PATH_STYLE_ACCESS}
      - STORAGE_BUCKET=${STORAGE_BUCKET}
      - AWS_REGION=${AWS_REGION}
      - GLUE_CATALOG_NAME=${GLUE_CATALOG_NAME}
      # Spark Configuration
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_DRIVER_MEMORY=1g
      - SPARK_EXECUTOR_MEMORY=1g
      - SPARK_EXECUTOR_CORES=1
    volumes:
      - ./data:/app/data:ro
      - ./output:/app/output
      - spark_logs:/opt/spark/logs
    ports:
      - "4040:4040"  # Spark Application UI
    networks:
      - spark-network
    restart: unless-stopped
    command: >
      sh -c "
        sleep 45 &&
        /opt/spark/bin/spark-submit \
          --master spark://spark-master:7077 \
          --deploy-mode client \
          --driver-memory 1g \
          --executor-memory 1g \
          --executor-cores 1 \
          --conf spark.sql.adaptive.enabled=true \
          --conf spark.sql.adaptive.coalescePartitions.enabled=true \
          --conf spark.sql.adaptive.skewJoin.enabled=true \
          --conf spark.sql.adaptive.localShuffleReader.enabled=true \
          --conf spark.sql.adaptive.optimizeSkewedJoin.enabled=true \
          --conf spark.sql.adaptive.forceApply=true \
          --conf spark.sql.adaptive.advisoryPartitionSizeInBytes=128m \
          --conf spark.sql.adaptive.coalescePartitions.minPartitionNum=1 \
          --conf spark.sql.adaptive.coalescePartitions.initialPartitionNum=200 \
          --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions \
          --conf spark.sql.catalog.spark_catalog=org.apache.iceberg.spark.SparkCatalog \
          --conf spark.sql.catalog.spark_catalog.type=hive \
          --conf spark.sql.catalog.spark_catalog.uri=thrift://hive-metastore:9083 \
          --conf spark.sql.catalog.spark_catalog.warehouse=s3a://spark-data/iceberg-warehouse \
          --conf spark.hadoop.aws.region=us-east-1 \
          --conf spark.hadoop.aws.access.key=minioadmin \
          --conf spark.hadoop.aws.secret.key=minioadmin \
          --conf spark.sql.catalog.glue_catalog=org.apache.iceberg.aws.glue.GlueCatalog \
          --conf spark.sql.catalog.glue_catalog.warehouse=s3a://spark-data/glue-warehouse \
          --conf spark.sql.catalog.glue_catalog.catalog-impl=org.apache.iceberg.aws.glue.GlueCatalog \
          --conf spark.sql.catalog.glue_catalog.io-impl=org.apache.iceberg.aws.s3.S3FileIO \
          --conf spark.sql.catalog.glue_catalog.s3.endpoint=http://minio:9000 \
          --conf spark.sql.catalog.glue_catalog.s3.access-key=minioadmin \
          --conf spark.sql.catalog.glue_catalog.s3.secret-key=minioadmin \
          --conf spark.hadoop.fs.s3a.endpoint=http://minio:9000 \
          --conf spark.hadoop.fs.s3a.access.key=minioadmin \
          --conf spark.hadoop.fs.s3a.secret.key=minioadmin \
          --conf spark.hadoop.fs.s3a.path.style.access=true \
          --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \
          --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider \
          --conf spark.sql.warehouse.dir=s3a://spark-data/warehouse \
          /app/main.py
      "

volumes:
  minio_data:
    driver: local
  spark_logs:
    driver: local
  spark_work:
    driver: local
  hive_metastore_db:
    driver: local

networks:
  spark-network:
    driver: bridge
